import tensorflow as tf
from data_load import load_vocab, load_data
from hyperparams import Hyperparams as hp

hangul2idx, idx2hangul, hanja2idx, idx2hanja = load_vocab()
X_train, Y_train, L_train = load_data(mode="train")
train_data_x = tf.data.Dataset.from_tensor_slices(X_train)
train_data_y = tf.data.Dataset.from_tensor_slices(Y_train)
train_data_l = tf.data.Dataset.from_tensor_slices(L_train)
dataset = tf.data.Dataset.zip((tf.data.Dataset.zip((train_data_x, train_data_y)), train_data_l)).shuffle(hp.buffer_size).batch(hp.batch_size)

for i, ((hangul, hanja), hanja_labels) in enumerate(dataset.take(1)):
    if i >= 3:
        break

"""
print(hangul.shape)
print(hanja.shape)
print(hanja_labels.shape)

print([idx2hangul[item] for item in hangul[0].numpy().tolist()])
print([idx2hanja[item] for item in hanja[0].numpy().tolist()])
print([idx2hanja[item] for item in hanja_labels[0].numpy().tolist()])

print(hanja[0][:10])
print(hanja_labels[0][:10])

from models import positional_encoding

pos_encoding = positional_encoding(length=2048, depth=512)

# Check the shape.
print(pos_encoding.shape)

import matplotlib.pyplot as plt

# Plot the dimensions.
plt.pcolormesh(pos_encoding.numpy().T, cmap='RdBu')
plt.ylabel('Depth')
plt.xlabel('Position')
plt.colorbar()
plt.show()
"""

from models import PositionalEmbedding

embed_hangul = PositionalEmbedding(vocab_size=len(hangul2idx), d_model=512)
embed_hanja = PositionalEmbedding(vocab_size=len(hanja2idx), d_model=512)

hangul_emb = embed_hangul(hangul)
hanja_emb = embed_hanja(hanja)

"""
print(hangul_emb[0, :10, :5])

from models import CrossAttention

sample_ca = CrossAttention(num_heads=2, key_dim=512)

print(hangul_emb.shape)
print(hanja_emb.shape)
ca = sample_ca(hanja_emb, hangul_emb)
print(ca.shape)

from models import GlobalSelfAttention

sample_gsa = GlobalSelfAttention(num_heads=2, key_dim=512)

print(hangul_emb.shape)
gsa = sample_gsa(hangul_emb)
print(gsa.shape)

from models import CausalSelfAttention

sample_csa = CausalSelfAttention(num_heads=2, key_dim=512)

print(hanja_emb.shape)
csa = sample_csa(hanja_emb)
print(csa.shape)

out1 = sample_csa(embed_hanja(hanja[:, :3]))
out1.shape
out2 = sample_csa(embed_hanja(hanja))[:, :3]
out2.shape
tf.reduce_max(abs(out1 - out2)).numpy()

from models import FeedForward

sample_ffn = FeedForward(512, 2048)

print(hanja_emb.shape)
print(sample_ffn(hanja_emb).shape)

from models import EncoderLayer

sample_encoder_layer = EncoderLayer(d_model=512, num_heads=8, dff=2048)

print(hangul_emb.shape)
print(sample_encoder_layer(hangul_emb).shape)

from models import Encoder

# Instantiate the encoder.
sample_encoder = Encoder(num_layers=4,
                         d_model=512,
                         num_heads=8,
                         dff=2048,
                         vocab_size=len(hangul2idx))

sample_encoder_output = sample_encoder(hangul, training=False)

# Print the shape.
print(hangul.shape)
print(sample_encoder_output.shape)  # Shape `(batch_size, input_seq_len, d_model)`

from models import DecoderLayer

sample_decoder_layer = DecoderLayer(d_model=512, num_heads=8, dff=2048)

sample_decoder_layer_output = sample_decoder_layer(
    x=hanja_emb, context=hangul_emb)

print(hanja_emb.shape)
print(hangul_emb.shape)
print(sample_decoder_layer_output.shape)  # `(batch_size, seq_len, d_model)`

from models import Decoder

# Instantiate the decoder.
sample_decoder = Decoder(num_layers=4,
                         d_model=512,
                         num_heads=8,
                         dff=2048,
                         vocab_size=len(hanja2idx))

output = sample_decoder(
    x=hanja,
    context=hangul_emb)

# Print the shapes.
print(hanja.shape)
print(hangul_emb.shape)
print(output.shape)

sample_decoder.last_attn_scores.shape  # (batch, heads, target_seq, input_seq)

from models import Transformer

transformer = Transformer(
    num_layers=hp.num_layers,
    d_model=hp.d_model,
    num_heads=hp.num_heads,
    dff=hp.dff,
    input_vocab_size=len(hangul2idx),
    target_vocab_size=len(hanja2idx),
    dropout_rate=hp.dropout_rate)

output = transformer((hangul, hanja))

print(hanja.shape)
print(hangul.shape)
print(output.shape)

attn_scores = transformer.decoder.dec_layers[-1].last_attn_scores
print(attn_scores.shape)  # (batch, heads, target_seq, input_seq)

transformer.summary()

import matplotlib.pyplot as plt
from models import CustomSchedule

learning_rate = CustomSchedule(hp.d_model)
optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,
                                     epsilon=1e-9)

plt.plot(learning_rate(tf.range(40000, dtype=tf.float32)))
plt.ylabel('Learning Rate')
plt.xlabel('Train Step')
plt.show()

X_train, Y_train, L_train = load_data(mode="train")
train_data_x = tf.data.Dataset.from_tensor_slices(X_train)
train_data_y = tf.data.Dataset.from_tensor_slices(Y_train)
train_data_l = tf.data.Dataset.from_tensor_slices(L_train)
train_batches = tf.data.Dataset.zip((tf.data.Dataset.zip((train_data_x, train_data_y)), train_data_l)).shuffle(hp.buffer_size).batch(hp.batch_size)
X_val, Y_val, L_val = load_data(mode="val")
val_data_x = tf.data.Dataset.from_tensor_slices(X_val)
val_data_y = tf.data.Dataset.from_tensor_slices(Y_val)
val_data_l = tf.data.Dataset.from_tensor_slices(L_val)
val_batches = tf.data.Dataset.zip((tf.data.Dataset.zip((val_data_x, val_data_y)), val_data_l)).shuffle(hp.buffer_size).batch(hp.batch_size)


m = H2HModel()
# Loading checkpoint
checkpoint_path = hp.logdir + "/cp.ckpt"
if not os.path.exists(hp.logdir):
    os.mkdir(hp.logdir)

if os.path.exists(checkpoint_path + ".index"):
    m.model.load_weights(checkpoint_path)
    keras.backend.set_value(m.model.optimizer.learning_rate, hp.learning_rate)

strings = [
    "그 성곽을 척량하매 일백 사십 사 규빗이니 사람의 척량 곧 천사의 척량이라",
    "그 열 두 문은 열 두 진주니 문마다 한 진주요 성의 길은 맑은 유리 같은 정금이더라",
]
article = strings

import numpy as np
import tensorflow as tf
from tensorflow import keras
from typing import List

from data_load import load_vocab_list, load_vocab
from hyperparams import Hyperparams as hp
from train import H2HModel

hanguls, hanjas = load_vocab_list()
hangul2idx, idx2hangul, hanja2idx, idx2hanja = load_vocab()

l_idx2hanja = keras.layers.StringLookup(
    vocabulary=hanjas,
    mask_token='', oov_token='[UNK]',
    invert=True
)
l_idx2hangul = keras.layers.StringLookup(
    vocabulary=hanguls,
    mask_token='', oov_token='[UNK]',
    invert=True
)

m = H2HModel()
checkpoint_path = hp.logdir + "/cp.ckpt"
m.model.load_weights(checkpoint_path)
model = m.model

start = 2
end = 3
hangul_indices = [
    [start] + [hangul2idx.get(char, 1) for char in line] + [end] + (hp.maxlen - len(line) - 2) * [0]
    for line in article
]
input_tensor = tf.convert_to_tensor(
    [tf.stack(indice) for indice in hangul_indices]
)
output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)
bulk_size = input_tensor.shape[0]
end_sentences = tf.cast(np.ones([bulk_size, 1]), dtype=tf.int64)
ends = tf.cast(np.array(end)[np.newaxis, np.newaxis], dtype=tf.int64)
for j in range(bulk_size):
    output_array = output_array.write(0 * bulk_size + j, start)

for i in tf.range(hp.maxlen):
    print(f"i is {i}")
    output = tf.transpose(tf.reshape(tf.transpose(output_array.stack()), [i + 1, bulk_size]))
    predictions = model([input_tensor, output], training=False)
    # Select the last token from the `seq_len` dimension.
    predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.
    predicted_id = tf.argmax(predictions, axis=-1)
    # Concatenate the `predicted_id` to the output which is given to the
    # decoder as its input.
    end_sentences *= tf.cast(predicted_id != ends, tf.int64)
    predicted_id *= end_sentences
    for j in range(bulk_size):
        output_array = output_array.write((i + 1) * bulk_size + j, predicted_id[j][0])
    if tf.reduce_sum(end_sentences) == 0:
        break


from predict import H2HPredictor
p = H2HPredictor()
string = "그 성곽을 척량하매 일백 사십 사 규빗이니 사람의 척량 곧 천사의 척량이라"
p(string)

strings = [
    "그 성곽을 척량하매 일백 사십 사 규빗이니 사람의 척량 곧 천사의 척량이라",
    "그 열 두 문은 열 두 진주니 문마다 한 진주요 성의 길은 맑은 유리 같은 정금이더라",
]
p.convert(strings)

import time
import codecs
strings = []
i = 0
for line in codecs.open('data/bible_ko.tsv', 'r', 'utf-8'):
    if len(line) <= 150:
        strings.append(line.strip().split("\t")[0])
        i += 1
    if i >= 1000:
        break

strings[962]

t = time.time()
c = p.convert(strings)
dt = time.time() - t
dt
c[962]

from models import BaseAttention

class PlainCrossAttention(BaseAttention):
    def call(self, x, context):
        attn_output, attn_scores = self.mha(
            query=x,
            key=context,
            value=context,
            return_attention_scores=True)
        # Cache the attention scores for plotting later.
        self.last_attn_scores = attn_scores
        return x

sample_ca = PlainCrossAttention(num_heads=2, key_dim=512)

print(hangul_emb.shape)
print(hanja_emb.shape)
ca = sample_ca(hanja_emb, hangul_emb)
print(ca.shape)

import numpy as np

ca_steps = []
for i in range(hanja_emb.shape[1]):
    ca_steps.append(sample_ca(hanja_emb[:, i: i + 1, :], hangul_emb))

ca_stepwise = tf.concat(ca_steps, axis=1)
tf.reduce_max(abs(ca_stepwise - ca)).numpy()


import numpy as np
import time
from models import CrossAttention
sample_ca = CrossAttention(num_heads=2, key_dim=512)

ca_uncached_1 = sample_ca(hanja_emb[:, :200, :], hangul_emb)
sample_ca.cache_reset()
ca_uncached_2 = sample_ca(hanja_emb, hangul_emb)
sample_ca.cache_reset()

t = time.time()
for i in range(200):
    ca = sample_ca(hanja_emb[:, : i + 1, :], hangul_emb, use_cache = True)

dt = time.time() - t

t = time.time()
for i in range(200):
    ca = sample_ca(hanja_emb[:, : i + 1, :], hangul_emb)

dt2 = time.time() - t

tf.reduce_max(abs(ca_uncached_1 - ca)).numpy()
tf.reduce_max(abs(ca_uncached_2 - ca)).numpy()


import numpy as np
import time
from models import CausalSelfAttention
sample_csa = CausalSelfAttention(num_heads=2, key_dim=512)

csa_uncached_1 = sample_csa(hanja_emb[:, :200, :])
sample_csa.cache_reset()
csa_uncached_2 = sample_csa(hanja_emb)
sample_csa.cache_reset()

t = time.time()
for i in range(200):
    csa = sample_csa(hanja_emb[:, : i + 1, :], use_cache = True)

dt = time.time() - t

t = time.time()
for i in range(200):
    csa = sample_csa(hanja_emb[:, : i + 1, :])

dt2 = time.time() - t

tf.reduce_max(abs(csa_uncached_1 - csa)).numpy()
tf.reduce_max(abs(csa_uncached_2 - csa)).numpy()

"""

