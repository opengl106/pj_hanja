import tensorflow as tf
from data_load import load_vocab, load_data
from hyperparams import Hyperparams as hp

hangul2idx, idx2hangul, hanja2idx, idx2hanja = load_vocab()
X_train, Y_train, L_train = load_data(mode="train")
train_data_x = tf.data.Dataset.from_tensor_slices(X_train)
train_data_y = tf.data.Dataset.from_tensor_slices(Y_train)
train_data_l = tf.data.Dataset.from_tensor_slices(L_train)
dataset = tf.data.Dataset.zip((tf.data.Dataset.zip((train_data_x, train_data_y)), train_data_l)).shuffle(hp.buffer_size).batch(hp.batch_size)

for i, ((hangul, hanja), hanja_labels) in enumerate(dataset.take(1)):
    if i >= 3:
        break

"""
print(hangul.shape)
print(hanja.shape)
print(hanja_labels.shape)

print([idx2hangul[item] for item in hangul[0].numpy().tolist()])
print([idx2hanja[item] for item in hanja[0].numpy().tolist()])
print([idx2hanja[item] for item in hanja_labels[0].numpy().tolist()])

print(hanja[0][:10])
print(hanja_labels[0][:10])

from models import positional_encoding

pos_encoding = positional_encoding(length=2048, depth=512)

# Check the shape.
print(pos_encoding.shape)

import matplotlib.pyplot as plt

# Plot the dimensions.
plt.pcolormesh(pos_encoding.numpy().T, cmap='RdBu')
plt.ylabel('Depth')
plt.xlabel('Position')
plt.colorbar()
plt.show()
"""

from models import PositionalEmbedding

embed_hangul = PositionalEmbedding(vocab_size=len(hangul2idx), d_model=512)
embed_hanja = PositionalEmbedding(vocab_size=len(hanja2idx), d_model=512)

hangul_emb = embed_hangul(hangul)
hanja_emb = embed_hanja(hanja)

"""
print(hangul_emb[0, :10, :5])

from models import CrossAttention

sample_ca = CrossAttention(num_heads=2, key_dim=512)

print(hangul_emb.shape)
print(hanja_emb.shape)
ca = sample_ca(hanja_emb, hangul_emb)
print(ca.shape)

from models import GlobalSelfAttention

sample_gsa = GlobalSelfAttention(num_heads=2, key_dim=512)

print(hangul_emb.shape)
gsa = sample_gsa(hangul_emb)
print(gsa.shape)

from models import CausalSelfAttention

sample_csa = CausalSelfAttention(num_heads=2, key_dim=512)

print(hanja_emb.shape)
csa = sample_csa(hanja_emb)
print(csa.shape)

out1 = sample_csa(embed_hanja(hanja[:, :3]))
out1.shape
out2 = sample_csa(embed_hanja(hanja))[:, :3]
out2.shape
tf.reduce_max(abs(out1 - out2)).numpy()

from models import FeedForward

sample_ffn = FeedForward(512, 2048)

print(hanja_emb.shape)
print(sample_ffn(hanja_emb).shape)

from models import EncoderLayer

sample_encoder_layer = EncoderLayer(d_model=512, num_heads=8, dff=2048)

print(hangul_emb.shape)
print(sample_encoder_layer(hangul_emb).shape)

from models import Encoder

# Instantiate the encoder.
sample_encoder = Encoder(num_layers=4,
                         d_model=512,
                         num_heads=8,
                         dff=2048,
                         vocab_size=len(hangul2idx))

sample_encoder_output = sample_encoder(hangul, training=False)

# Print the shape.
print(hangul.shape)
print(sample_encoder_output.shape)  # Shape `(batch_size, input_seq_len, d_model)`

from models import DecoderLayer

sample_decoder_layer = DecoderLayer(d_model=512, num_heads=8, dff=2048)

sample_decoder_layer_output = sample_decoder_layer(
    x=hanja_emb, context=hangul_emb)

print(hanja_emb.shape)
print(hangul_emb.shape)
print(sample_decoder_layer_output.shape)  # `(batch_size, seq_len, d_model)`

from models import Decoder

# Instantiate the decoder.
sample_decoder = Decoder(num_layers=4,
                         d_model=512,
                         num_heads=8,
                         dff=2048,
                         vocab_size=len(hanja2idx))

output = sample_decoder(
    x=hanja,
    context=hangul_emb)

# Print the shapes.
print(hanja.shape)
print(hangul_emb.shape)
print(output.shape)

sample_decoder.last_attn_scores.shape  # (batch, heads, target_seq, input_seq)

from models import Transformer

transformer = Transformer(
    num_layers=hp.num_layers,
    d_model=hp.d_model,
    num_heads=hp.num_heads,
    dff=hp.dff,
    input_vocab_size=len(hangul2idx),
    target_vocab_size=len(hanja2idx),
    dropout_rate=hp.dropout_rate)

output = transformer((hangul, hanja))

print(hanja.shape)
print(hangul.shape)
print(output.shape)

attn_scores = transformer.decoder.dec_layers[-1].last_attn_scores
print(attn_scores.shape)  # (batch, heads, target_seq, input_seq)

transformer.summary()

import matplotlib.pyplot as plt
from models import CustomSchedule

learning_rate = CustomSchedule(hp.d_model)
optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,
                                     epsilon=1e-9)

plt.plot(learning_rate(tf.range(40000, dtype=tf.float32)))
plt.ylabel('Learning Rate')
plt.xlabel('Train Step')
plt.show()

"""